---
ai_commentary: []
ai_commentary_meta:
  content_digest: ''
  generated_at: ''
  model: ''
  prompt_version: ''
  provider: ''
category: ai-research
date: '2025-11-12T22:51:26.344223+00:00'
exported_at: '2025-11-12T22:54:41.537156+00:00'
feed: http://feeds.feedburner.com/blogspot/gJZg
language: en
source_url: http://blog.research.google/2024/03/screenai-visual-language-model-for-ui.html
structured_data:
  about: []
  author: ''
  description: 'ScreenAI: A visual language model for UI and visually-situated language
    understanding'
  headline: 'ScreenAI: A visual language model for UI and visually-situated language
    understanding'
  inLanguage: en
  keywords: []
  main_image: ''
  original_source: http://blog.research.google/2024/03/screenai-visual-language-model-for-ui.html
  publisher:
    logo: /favicon.ico
    name: gtcode.com
title: 'ScreenAI: A visual language model for UI and visually-situated language understanding'
updated_at: '2025-11-12T22:51:26.344223+00:00'
url_hash: 4a0bc5fa7692e4af5827d31210bf8949cf40499d
---

*This project is the result of joint work with Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Carbune, Jason Lin, Jindong Chen and Abhanshu Sharma. We thank Fangyu Liu, Xi Chen, Efi Kokiopoulou, Jesse Berent, Gabriel Barcik, Lukas Zilka, Oriana Riva, Gang Li,Yang Li, Radu Soricut, and Tania Bedrax-Weiss for their insightful feedback and discussions, along with Rahul Aralikatte, Hao Cheng and Daniel Kim for their support in data preparation. We also thank Jay Yagnik, Blaise Aguera y Arcas, Ewa Dominowska, David Petrou, and Matt Sharifi for their leadership, vision and support. We are very grateful toTom Small for helping us create the animation in this post.*