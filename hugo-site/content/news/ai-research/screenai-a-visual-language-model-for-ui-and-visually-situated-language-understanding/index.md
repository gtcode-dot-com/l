---
ai_commentary:
- body: Introduces ScreenAI as a visual language model for UI and visually-situated
    language understanding, emphasizing cross-modal reasoning in UI contexts.
  title: Project scope
- body: Acknowledges the core team and collaborators who contributed to the work,
    including the individuals named in the acknowledgments.
  title: Contributors and collaborators
- body: Notes extensive feedback, discussions, data preparation assistance, and leadership
    from a broad set of colleagues.
  title: Acknowledgments and support
- body: Credits Tom Small for helping create the animation accompanying the post.
  title: Visualization and animation
ai_commentary_meta:
  content_digest: aa9f493eb35fa3e5cacbdf1515177577907ba079
  generated_at: '2025-11-10T01:58:11.284476+00:00'
  model: gpt-5-nano-2025-08-07
  prompt_version: v2025-11-09
  provider: openai
category: ai-research
date: '2025-11-09T05:13:21.724722+00:00'
exported_at: '2025-11-09T05:30:20.833370+00:00'
feed: http://feeds.feedburner.com/blogspot/gJZg
source_url: http://blog.research.google/2024/03/screenai-visual-language-model-for-ui.html
structured_data:
  about: &id001
  - Represents ScreenAI, a visual language model for user interfaces and visually-situated
    language understanding, as a collaborative AI-research effort.
  - Core contributors include Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter,
    Victor Carbune, Jason Lin, Jindong Chen, and Abhanshu Sharma, among others.
  - Acknowledges feedback and discussions from numerous colleagues (Fangyu Liu, Xi
    Chen, Efi Kokiopoulou, Jesse Berent, Gabriel Barcik, Lukas Zilka, Oriana Riva,
    Gang Li, Yang Li, Radu Soricut, Tania Bedrax-Weiss) and data-preparation support
    (Rahul Aralikatte, Hao Cheng, Daniel Kim).
  - Recognizes leadership and support from Jay Yagnik, Blaise Aguera y Arcas, Ewa
    Dominowska, David Petrou, and Matt Sharifi; animation assistance by Tom Small.
  description: A collaborative AI-research project introducing ScreenAI, a visual
    language model designed for UI contexts and visually-situated language understanding.
    The work credits a broad team of researchers and supporters for feedback, data
    preparation, leadership, and animation.
  headline: 'ScreenAI: A visual language model for UI and visually-situated language
    understanding'
  keywords: *id001
title: 'ScreenAI: A visual language model for UI and visually-situated language understanding'
updated_at: '2025-11-09T05:13:21.724722+00:00'
url_hash: 4a0bc5fa7692e4af5827d31210bf8949cf40499d
---

*This project is the result of joint work with Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Carbune, Jason Lin, Jindong Chen and Abhanshu Sharma. We thank Fangyu Liu, Xi Chen, Efi Kokiopoulou, Jesse Berent, Gabriel Barcik, Lukas Zilka, Oriana Riva, Gang Li,Yang Li, Radu Soricut, and Tania Bedrax-Weiss for their insightful feedback and discussions, along with Rahul Aralikatte, Hao Cheng and Daniel Kim for their support in data preparation. We also thank Jay Yagnik, Blaise Aguera y Arcas, Ewa Dominowska, David Petrou, and Matt Sharifi for their leadership, vision and support. We are very grateful toTom Small for helping us create the animation in this post.*