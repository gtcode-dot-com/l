---
ai_commentary: []
ai_commentary_meta:
  content_digest: ''
  generated_at: ''
  model: ''
  prompt_version: ''
  provider: ''
category: ai-research
date: '2026-01-27T18:15:30.369918+00:00'
exported_at: '2026-01-27T18:15:32.604178+00:00'
feed: https://huggingface.co/blog/feed.xml
source_url: https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment
structured_data:
  about: []
  author: ''
  description: A Blog post by Hugging Face on Hugging Face
  headline: One Year Since the “DeepSeek Moment”
  keywords: []
  main_image: ''
  original_source: https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment
  publisher:
    logo: /favicon.ico
    name: gtcode.com
title: One Year Since the “DeepSeek Moment”
updated_at: '2026-01-27T18:15:30.369918+00:00'
url_hash: 4c66d465692b0cab74c0d1ebb208fc07730b9967
---

# One Year Since the “DeepSeek Moment”

This is the first blog in a series that will examine China’s open source community’s historical advancements in the past year and its reverberations in shaping the entire ecosystem. Much of 2025’s progress can be traced back to January’s “DeepSeek Moment”, when Hangzhou-based AI company DeepSeek released their R-1 model.

The first blog addresses strategic changes and the explosion of new open models and open source players. The second will cover architectural and hardware choices largely by Chinese companies made in the wake of a growing open ecosystem. The third will analyze prominent organizations’ trajectories and the future of the global open source ecosystem.

For AI researchers and developers contributing to and relying on the open source ecosystem and for policymakers understanding the rapidly changing environment,
**there has never been a better time to build and release open models and artifacts, as proven by the past year’s immense growth catalyzed by DeepSeek. Notably, geopolitics has driven adoption; while models developed in China have been dominating across metrics throughout 2025 and new players leapfrogging each other, Western AI communities are seeking commercially deployable alternatives.**

## **The Seeds of China’s Organic Open Source AI Ecosystem**

Before R1, China’s AI industry was still largely centered on closed models. Open models had existed for years, but they were mostly confined to research communities or used only in niche scenarios such as privacy-sensitive applications. For most companies, they were not the default choice. Compute resources were tight, and whether to “open or close” was a subject of debate.

[DeepSeek’s R1 model](https://huggingface.co/deepseek-ai/DeepSeek-R1)
lowered the barrier to advanced AI capabilities and offered a clear pattern to follow, unlocking a second layer. Moreover, the release gave Chinese AI development something extremely valuable: time. It showed that even with limited resources, rapid progress was still possible through open source and fast iteration. This approach aligned naturally with the goals set out in China’s 2017 “AI+” strategy: combining AI with industry as early as possible, while continuing to build up compute capacity over the long term.

One year after the release of R1, what we see emerging is not only a collection of new models, but also a growing organic open source AI ecosystem.

## **DeepSeek R1: A Turning Point**

For the first time, an open model from China entered the global mainstream rankings and, over the following year, was repeatedly used as a reference point when new models were released. DeepSeek’s R1 quickly became the most liked model on Hugging Face of all time and the top liked models are no longer majority U.S.-developed.

[![Most Liked HF Models DS](https://cdn-uploads.huggingface.co/production/uploads/62543749b777cd32720675c2/VDKxdVg_gHi0EXOqG0VMk.png)](https://cdn-uploads.huggingface.co/production/uploads/62543749b777cd32720675c2/VDKxdVg_gHi0EXOqG0VMk.png)

**But R1’s real significance was not whether it was the strongest model at the time, its importance lay in how it lowered three barriers.**

**The first was the technical barrier.**
By openly
[sharing](https://www.nature.com/articles/s41586-025-09422-z)
its reasoning paths and post-training methods, R1 turned advanced reasoning, previously locked behind closed APIs, into an engineering asset that could be downloaded, distilled, and fine-tuned. Many teams no longer needed to train massive models from scratch to gain strong reasoning capabilities. Reasoning started to behave like a reusable module, applied again and again across different systems. This also pushed the industry to rethink the relationship between model capability and compute cost, a shift that was especially meaningful in a compute-constrained environment like China.

**The second was the adoption barrier.**
R1 was released under the MIT license, making its use, modification, and redistribution straightforward. Companies that had relied on closed models began bringing R1 directly into production. Distillation, secondary training, and domain-specific adaptation became routine engineering work rather than special projects. As distribution constraints fell away, the model quickly spread into cloud platforms and toolchains, and community discussions shifted from “which model scores higher” to “how do we deploy it, reduce cost, and integrate it into real systems.” Over time, R1 moved beyond being a research artifact and became a reusable engineering foundation.

**The third change was psychological.**
When the question shifted from “can we do this?” to “how do we do this well?”, decision-making across many companies changed. For the Chinese AI community, this was also a rare moment of sustained global attention, one that mattered deeply to an ecosystem long seen mainly as a follower.

Together, the lowering of these three barriers meant that the ecosystem began to gain the ability to replicate itself.

## **From DeepSeek to AI+: Strategic Realignmentt**

Once open source moved into the mainstream, a natural question followed: how would Chinese companies' strategies change?Over the past year, the answer became clear: competition began to shift from model-to-model comparisons toward system level capabilities.

Compared with 2024, the period after the release of R1 saw China’s AI landscape settle into a new pattern. Large technology companies took the lead, startups followed quickly, and companies from vertical industries increasingly entered the field. While their paths differed, a shared understanding gradually emerged, especially among leading players:
**open source was no longer a short-term tactic, but part of long-term competitive strategy.**

[![HG Repository Growth(1)](https://cdn-uploads.huggingface.co/production/uploads/62543749b777cd32720675c2/SOdceKfaC3wNuoVorEHtx.png)](https://cdn-uploads.huggingface.co/production/uploads/62543749b777cd32720675c2/SOdceKfaC3wNuoVorEHtx.png)

**The number of competitive Chinese organizations releasing state of the art models and repositories skyrocketed.**
Reflected in
*Hugging Face Repository Growth of Chinese Companies*
, the number of open releases from existing giants substantially increased, with
[Baidu](https://huggingface.co/baidu)
going from zero releases on Hugging Face in 2024 to over 100 in 2025, and others such as
[ByteDance](https://huggingface.co/ByteDance)
and
[Tencent](https://huggingface.co/tencent)
increasing releases by eight to nine times. An influx of newly open organizations released highly performant models, with
[Moonshot](https://huggingface.co/moonshotai)
’s open release,
[Kimi K2](https://huggingface.co/moonshotai/Kimi-K2-Thinking)
, being a “
[another DeepSeek moment](https://www.interconnects.ai/p/kimi-k2-and-when-deepseek-moments)
”.

[![Top New Models](https://cdn-uploads.huggingface.co/production/uploads/62543749b777cd32720675c2/BX23GdUzkxQuMNlQ0YfHx.png)](https://cdn-uploads.huggingface.co/production/uploads/62543749b777cd32720675c2/BX23GdUzkxQuMNlQ0YfHx.png)

**Releases became stronger and frequent, with performant models released on a weekly basis;**
newly created Chinese models consistently became most liked and downloaded every week, boasting highest popularity among the top most downloaded new models on Hugging Face. The
*Top Newly Created Models by Week on Hugging Face*
shows new repositories labeled by organization location or base model organization location for popular derivatives.

As seen in Hugging Face’s
[heatmap data](https://huggingface.co/spaces/zh-ai-community/model-release-heatmap-zh)
, between February and July 2025, open releases from Chinese companies became noticeably more active. Baidu and Moonshot moved from primarily closed approaches toward open release.
[Zhipu AI](https://huggingface.co/zai-org)
’s
[GLM](https://huggingface.co/collections/zai-org/glm-47)
and Alibaba’s
[Qwen](https://huggingface.co/Qwen)
went a step further, expanding from simply publishing model weights to building engineering systems and ecosystem interfaces. At this stage, comparing raw model performance alone was no longer enough to win. Competition increasingly centered on ecosystems, application scenarios, and infrastructure\_.\_

This strategy was effectively successful; of newly created models (<1 year), downloads for Chinese models have surpassed any other country including the U.S.

[![Downloads_2025](https://cdn-uploads.huggingface.co/production/uploads/62543749b777cd32720675c2/Dp1rUOEVD_70BcHqaX5Sc.png)](https://cdn-uploads.huggingface.co/production/uploads/62543749b777cd32720675c2/Dp1rUOEVD_70BcHqaX5Sc.png)

Chinese AI players are not coordinating by agreement, but by constraint.
**What looks like collaboration is better understood as alignment under shared technical, economic, and regulatory pressures.**
This does not mean companies formed cooperative alliances. Rather, under similar constraints around compute, cost, and compliance, they began competing along similar technical foundations and engineering paths. When competition takes place on comparable system structures, the ecosystem starts to show the ability to spread and grow on its own. Tech leaders from Zhipu AI (Z.ai), Moonshot AI, Alibaba’s Qwen, and Tencent coordinating on shared questions is rarely seen in other countries.

## **Global Reception and Response**

Positive sentiment toward open source adoption and development has increased worldwide and especially in the U.S., with broader recognition of how open source leadership is critical in global competitiveness.

DeepSeek has been
[heavily adopted](https://www.microsoft.com/en-us/corporate-responsibility/topics/ai-economy-institute/reports/global-ai-adoption-2025/)
in global markets, especially in Southeast Asia and Africa.  In these markets, factors such as multilingual support, open-weight availability, and cost considerations have supported enterprise use.

**Often Western organizations seek non-Chinese models for commercial deployment.**
Major releases from U.S. organizations such as
[OpenAI’s gpt-oss](https://huggingface.co/collections/openai/gpt-oss)
,
[AI2’s Olmo](https://huggingface.co/collections/allenai/olmo-31)
, and
[Meta’s Llama 4](https://huggingface.co/collections/meta-llama/llama-4)
received community engagement. Reflection AI
[announced](https://techcrunch.com/2025/10/09/reflection-raises-2b-to-be-americas-open-frontier-ai-lab-challenging-deepseek/)
its efforts to build frontier American open-weight models. In France, Mistral released their
[Mistral Large 3 family](https://huggingface.co/collections/mistralai/mistral-large-3)
, continuously developing their open source roots.

At the same time, major releases in the West build on Chinese models; in November 2025, Deep Cogito released
[Cogito v2.1](https://huggingface.co/blog/deepcogito/cogito-v2-1)
as the leading U.S. open-weight model. The
[model](https://huggingface.co/deepcogito/cogito-671b-v2.1)
was a fine-tuned version of DeepSeek-V3.
**Startups and researchers globally using open-weight models are often defaulting to if not relying on models developed in China.**

The
[American Truly Open Model (ATOM) project](https://www.atomproject.ai/)
cites DeepSeek and China’s model momentum as a motivator for concerted efforts toward leading in open-weight model development. The project emphasizes the need for multiple efforts and its research also highlights OpenAI’s
[gpt-oss heavy early adoption](https://atomproject.ai/relative-adoption-metric)
.

The world is still responding, with a new open source fervor. 2026 is shaping up for major releases , especially from China and the U.S. Of high relevance are the architectural trends, hardware choices, and organizational directions, which will be covered next in this series.

All data represented is sourced from
[Hugging Face](https://huggingface.co/datasets/cfahlgren1/hub-stats)
. For more related data and analyses of 2025 in open source, we encourage you to read the Data Provenance Initiative and Hugging Face’s
[Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem](https://www.dataprovenance.org/economies-of-open-intelligence.pdf)
, aiWorld’s
[Open Source AI Year In Review 2025](https://huggingface.co/spaces/aiworld-eu/Open-Source-AI-Year-in-Review-2025)
, and InterConnects’s
[8 plots that explain the state of open models](https://www.interconnects.ai/p/8-plots-that-explain-the-state-of)
.