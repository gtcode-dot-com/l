---
ai_commentary: []
ai_commentary_meta:
  content_digest: ''
  generated_at: ''
  model: ''
  prompt_version: ''
  provider: ''
category: ai-research
date: '2025-11-18T02:20:58.797139+00:00'
exported_at: '2025-11-18T02:21:00.080116+00:00'
feed: http://feeds.feedburner.com/nvidiablog
language: en
source_url: https://blogs.nvidia.com/blog/accelerated-computing-networking-supercomputing-ai
structured_data:
  about: []
  author: ''
  description: At SC25, NVIDIA unveiled advances across NVIDIA BlueField DPUs, next-generation
    networking, quantum computing, national research, AI physics and more ‚Äî as accelerated
    systems drive the next chapter in AI supercomputing. NVIDIA also highlighted storage
    innovations powered by the NVIDIA BlueField-4 data processing unit, part of the
    full-stack BlueField platform that accelerates gigascale AI infrastructure. More  Read
    Article
  headline: Accelerated Computing, Networking Drive Supercomputing in Age of AI
  inLanguage: en
  keywords: []
  main_image: ''
  original_source: https://blogs.nvidia.com/blog/accelerated-computing-networking-supercomputing-ai
  publisher:
    logo: /favicon.ico
    name: gtcode.com
title: Accelerated Computing, Networking Drive Supercomputing in Age of AI
updated_at: '2025-11-18T02:20:58.797139+00:00'
url_hash: 06d881956577ccef2c6660bd8d809ffe4698ffba
---

At SC25, NVIDIA unveiled advances across NVIDIA BlueField DPUs, next-generation networking, quantum computing, national research, AI physics and more ‚Äî as accelerated systems drive the next chapter in AI supercomputing.

![](https://blogs.nvidia.com/wp-content/uploads/2025/11/SC25-Fireside-Chat-DEBM5955-1680x1120.jpg)


Ian Buck, vice president and general manager of accelerated computing at NVIDIA, delivered a special address at SC25.

NVIDIA also highlighted storage innovations powered by the
[NVIDIA BlueField-4 data processing unit](https://www.nvidia.com/en-us/networking/products/data-processing-unit/)
, part of the full-stack BlueField platform that accelerates gigascale AI infrastructure.

More details also came on NVIDIA Quantum-X Photonics InfiniBand CPO networking switches ‚Äî enabling AI factories to drastically reduce energy consumption and operational costs ‚Äî including that TACC, Lambda and CoreWeave plan to integrate them.

Last month, NVIDIA began shipping DGX Spark, the world‚Äôs smallest AI supercomputer. DGX Spark packs a petaflop of AI performance and 128GB of unified memory into a desktop form factor, enabling developers to run inference on models up to 200 billion parameters and fine-tune models locally. Built on the Grace Blackwell architecture, it integrates NVIDIA GPUs, CPUs, networking, CUDA libraries and the full NVIDIA AI software stack.

DGX Spark‚Äôs unified memory and NVIDIA NVLink-C2C deliver 5x the bandwidth of PCIe Gen5, enabling faster GPU-CPU data exchange. This boosts training efficiency for large models, reduces latency and supports seamless fine-tuning workflows ‚Äî all within a desktop form factor.

## **NVIDIA Apollo Unveiled as Latest Open Model Family for AI Physics**

[NVIDIA Apollo](https://blogs.nvidia.com/blog/apollo-open-models)
, a family of open models for AI Physics, was also introduced at SC25. Applied Materials, Cadence, LAM Research, Luminary Cloud, KLA, PhysicsX, Rescale, Siemens and Synopsys ¬†are among the industry leaders adopting these open models to simulate and accelerate their design processes in a broad range of fields ‚Äî electronic device automation and semiconductors, computational fluid dynamics, structural mechanics, electromagnetics, weather and more.

The family of open models harness the latest developments in AI physics, incorporating best-in-class machine learning architectures, such as neural operators, transformers and diffusion methods, with domain-specific knowledge. Apollo will provide pretrained checkpoints and reference workflows for training, inference and benchmarking, allowing developers to integrate and customize the models for their specific needs.

![](https://blogs.nvidia.com/wp-content/uploads/2025/11/industrial-manufacturing-social-cae-ai-physics-open-model-blog-1280x680.jpg-960x510.png)

## **NVIDIA Warp Supercharges Physics Simulations‚Äã [üîó](https://blogs.nvidia.com/blog/accelerated-computing-networking-supercomputing-ai/#warp)**

[NVIDIA Warp](https://developer.nvidia.com/warp-python)
is a purpose-built open-source Python framework delivering GPU acceleration for computational physics and AI by up to 245x.

NVIDIA Warp provides a structured approach for simulation, robotics and machine learning workloads, combining the accessibility of Python with performance comparable to native CUDA code.

Warp supports the creation of GPU-accelerated 3D simulation workflows that integrate with ML pipelines in PyTorch, JAX, NVIDIA PhysicsNeMo and NVIDIA Omniverse. This allows developers to run complex simulation tasks and generate data at scale without leaving the Python programming environment.

By offering CUDA-level performance with Python-level productivity, Warp simplifies the development of high-performance simulation workflows. It is designed to accelerate AI research and engineering by reducing barriers to GPU programming, making advanced simulation and data generation more efficient and widely accessible.

Siemens, Neural Concept, Luminary Cloud, among others, are adopting NVIDIA Warp.

![NVIDIA BlueField-4 DPU powering the OS of AI factories ](https://blogs.nvidia.com/wp-content/uploads/2025/11/bluefield-corp-blog-bluefield-4-1280x680-4468150-960x510.png)


NVIDIA BlueField-4 DPU: The Processor Powering the Operating System of AI Factories

## **Showcasing BlueField-4 for Powering the OS of AI Factories [üîó](https://blogs.nvidia.com/blog/accelerated-computing-networking-supercomputing-ai/#bluefield-4)**

Unveiled at GTC Washington, D.C.,
[NVIDIA BlueField-4 DPUs](https://www.nvidia.com/en-us/networking/products/data-processing-unit/)
are powering the operating system of AI factories. By offloading, accelerating and isolating critical data center functions ‚Äî networking, storage and security ‚Äî they free up CPUs and GPUs to focus entirely on compute-intensive workloads.

[BlueField-4](https://resources.nvidia.com/en-us-accelerated-networking-resource-library/bluefield-4-dpu-datasheet)
, combining a 64-core NVIDIA Grace CPU and
[NVIDIA ConnectX-9](https://resources.nvidia.com/en-us-accelerated-networking-resource-library/connectx-9-supernic-datasheet)
networking, unlocks unprecedented performance, efficiency and zero-trust security at scale. It supports multi-tenant environments, rapid data access, and real-time protection, with native integration of
[NVIDIA DOCA](https://developer.nvidia.com/networking/doca)
microservices for scalable, containerized AI operations. Together, they are transforming data centers into intelligent, software-defined engines for trillion-token AI and beyond.

As AI factories and supercomputing centers continue to scale in size and capability, they require faster, more intelligent storage infrastructure to manage structured, unstructured and AI-native data for large-scale training and inference.

Leading storage innovators ‚Äî DDN, VAST Data and WEKA ‚Äî are adopting BlueField-4 to redefine performance and efficiency for AI and scientific workloads.

* [DDN](https://www.ddn.com/press-releases/ddn-powers-the-next-generation-of-ai-factories-with-nvidia-bluefield-4/)
  is building next-generation AI factories, accelerating data pipelines to maximize GPU utilization for AI and HPC workloads.
* [VAST Data](https://www.vastdata.com/blog/advancing-the-state-of-the-art-with-nvidia-blue-field-4)
  is advancing the AI pipeline with intelligent data movement and real-time efficiency across large-scale AI clusters.
* [WEKA](https://www.weka.io/company/weka-newsroom/press-releases/weka-announces-new-neuralmesh-architecture-for-nvidia-bluefield-4/)
  is launching its NeuralMesh architecture on BlueField-4, running storage services directly on the DPU to simplify and accelerate AI infrastructure.

Together, these HPC storage leaders are demonstrating how NVIDIA BlueField-4 transforms data movement and management ‚Äî turning storage into a performance multiplier for the next era of supercomputing and AI infrastructure.

![NVIDIA ConnectX-9 SuperNIC](https://blogs.nvidia.com/wp-content/uploads/2025/11/infiniband-corp-blog-connectx-9-supernic-c9180-1280x680-1-960x510.jpg)


NVIDIA ConnectX-9 SuperNIC

## **Adopting NVIDIA Co-Packaged Optics for Speed and Reliability‚Äã [üîó](https://blogs.nvidia.com/blog/accelerated-computing-networking-supercomputing-ai/#co-packaged-optics)**

TACC, Lambda and CoreWeave unveiled that they will integrate NVIDIA Quantum-X Photonics CPO switches into next generation systems as early as next year.

NVIDIA Quantum-X Photonics networking switches enable AI factories and supercomputing centers to drastically reduce energy consumption and operational costs. NVIDIA has achieved this fusion of electronic circuits and optical communications at massive scale.

VIDEO

As AI factories grow to unprecedented sizes, networks must evolve to keep pace. By eliminating traditional pluggable transceivers, a common cause of job runtime failures, NVIDIA Photonics switch systems not only deliver 3.5x better power efficiency, but also perform with 10x higher resiliency, enabling applications to run 5x longer without interruption.

At GTC 2024 in Silicon Valley, NVIDIA unveiled NVIDIA Quantum-X800 InfiniBand switches, purpose-built to power trillion-parameter-scale generative AI models. These platforms deliver a staggering 800Gb/s end-to-end throughput ‚Äî 2x the bandwidth and 9x the in-network compute of their predecessors ‚Äî owing to such innovations as SHARPv4 and FP8 support.

As NVIDIA Quantum‚ÄëX800 continues to be widely adopted to meet the demands of massive-scale AI, NVIDIA Quantum‚ÄëX Photonics, announced at GTC earlier this year, addresses the critical power, resiliency, and signal-integrity challenges of even larger deployments. By integrating optics directly on the switch, it eliminates failures caused by pluggable transceivers and link flaps, enabling workloads to run uninterrupted at scale and ensuring the infrastructure can support the next generation of compute-intensive applications up to 5x better than with pluggable transceivers.

‚ÄúNVIDIA Quantum‚ÄëX Photonics represents the next step in building high-performance, resilient AI networks,‚Äù said Maxx Garrison, product manager for cloud infrastructure at Lambda. ‚ÄúThese advances in power efficiency, signal integrity and reliability, will be key to supporting efficient, large-scale workloads for our customers.‚Äù

SHARPv4 enables in-network aggregation and reduction, minimizing GPU-to-GPU communication overhead. Combined with FP8 precision, it accelerates training of trillion-parameter models by reducing bandwidth and compute demands ‚Äî delivering faster convergence and higher throughput and comes standard with NVIDIA Quantum‚ÄëX800 and Quantum‚ÄëX Photonics switches.

‚ÄúCoreWeave is building the Essential Cloud for AI,‚Äù said Peter Salanki, co-founder and chief technology officer at CoreWeave. ‚ÄúWith NVIDIA Quantum-X Photonics, we‚Äôre advancing power efficiency, and further improving the reliability CoreWeave is known for in supporting massive AI workloads at scale, helping our customers unlock the full potential of next-generation AI.‚Äù

The NVIDIA Quantum-X Photonics platform, anchored by the NVIDIA Quantum Q3450 CPO-based InfiniBand switch and ConnectX-8 SuperNIC, is engineered for the highest-performance environments that also require significantly lower power, higher resiliency and lower latency.

## **Supercomputing Centers Worldwide Adopting NVQLink**

![](https://blogs.nvidia.com/wp-content/uploads/2025/11/NVIDIANVQLink.jpg)

More than a dozen of the world‚Äôs top scientific computing centers are adopting
[NVQLink](https://nvidianews.nvidia.com/news/scientific-supercomputing-centers-nvqlink-grace-blackwell-quantum-processors)
, a universal interconnect linking accelerated computing to quantum processors.

‚ÄúHere at Supercomputing, we‚Äôre announcing that we‚Äôve been working with the supercomputing centers worldwide that are dedicated and interested in building the next generation of quantum GPU, CPU GPU supercomputers, and how to connect them to their particular research area or deployment platform for quantum computing,‚Äù said Ian Buck, vice president and general manager of accelerated computing at NVIDIA.

NVQLink connects quantum processors with NVIDIA GPUs, enabling large‚Äëscale workflows powered by the CUDA‚ÄëQ software platform. NVQLink‚Äôs open architecture provides the critical link supercomputing centers need to integrate diverse quantum processors while delivering 40 petaflops of AI performance at FP4 precision.

In the future every supercomputer will draw on quantum processors to expand the problems they can solve and every quantum processor will depend on GPU supercomputers to run correctly.

Quantum computing company Quantinuum‚Äôs new Helios QPU was integrated with NVIDIA GPUs through NVQLink, achieving the world‚Äôs first real‚Äëtime decoding of scalable qLDPC quantum error‚Äëcorrection codes. The system maintained 99% fidelity compared with 95% without correction thanks to NVQLink‚Äôs microsecond low latencies.

With NVQLink scientists and developers gain a universal bridge between quantum and classical hardware ‚Äî making scalable error correction, hybrid applications and real‚Äëtime quantum‚ÄëGPU workflows practical.

In the Asia‚ÄëPacific region, Japan‚Äôs Global Research and Development Center for Business by Quantum-AI technology (G-QuAT) at the National Institute of Advanced Industrial Science and Technology (AIST) and RIKEN Center for Computational Science, Korea‚Äôs Korea Institute of Science and Technology Information (KISTI), Taiwan‚Äôs National Center for High-Performance Computing (NCHC), Singapore‚Äôs National Quantum Computing Hub (a joint initiative of Singapore‚Äôs Centre for Quantum Technologies, A\*STAR Institute of High Performance Computing, and National Supercomputing Centre Singapore) ‚Äî and Australia‚Äôs Pawsey Supercomputing Research Centre are among the early adopters.

Across Europe and the Middle East, NVQLink is being embraced by CINECA, Denmark‚Äôs DCAI, operator of Denmark‚Äôs AI Supercomputer, France‚Äôs Grand √âquipement National de Calcul Intensif (GENCI), the Czech Republic‚Äôs IT4Innovations National Supercomputing Center (IT4I), Germany‚Äôs J√ºlich Supercomputing Centre (JSC), Poland‚Äôs Pozna≈Ñ Supercomputing and Networking Center (PCSS), the Technology Innovation Institute (TII), UAE and Saudi Arabia‚Äôs King Abdullah University of Science and Technology (KAUST).

In the United States, leading national laboratories including, Brookhaven National Laboratory, Fermi National Accelerator Laboratory, Lawrence Berkeley National Laboratory, Los Alamos National Laboratory, MIT Lincoln Laboratory, National Energy Research Scientific Computing Center, Oak Ridge National Laboratory, Pacific Northwest National Laboratory and Sandia National Laboratories are also adopting NVQLink to advance hybrid quantum‚Äëclassical research.

## **Developing Real‚ÄëWorld Hybrid Applications**

Quantinuum‚Äôs Helios QPU with NVQLink delivered:

* First real‚Äëtime decoding of qLDPC error‚Äëcorrection codes
* ~99% fidelity with NVQLink correction vs ~95% without
* Reaction time of 60 microseconds, exceeding Helios‚Äô 1‚Äëmillisecond requirement by 16x

NVQLink unites quantum processors with GPU supercomputing for scalable error correction and hybrid applications. Scientists can gain a single programming environment through CUDA‚ÄëQ APIs. Developers can build and test quantum‚ÄëGPU workflows in real time

With NVQLink the world‚Äôs supercomputing centers are laying the foundation for practical quantum‚Äëclassical systems, connecting diverse quantum processors to NVIDIA accelerated computing at unprecedented speed and scale.

## **NVIDIA and** **RIKEN** **Advance Japan‚Äôs Scientific Frontiers**

![](https://blogs.nvidia.com/wp-content/uploads/2025/11/hpc-corp-blog-sc25-riken-pr-1280x680-2.jpg)

NVIDIA and RIKEN are building
[two new GPU‚Äëaccelerated supercomputers](https://nvidianews.nvidia.com/news/nvidia-and-riken-advance-japans-scientific-frontiers-with-new-supercomputers-for-ai-and-quantum-computing)
to expand Japan‚Äôs leadership in AI for science and quantum computing. Together the systems will feature 2,140 NVIDIA Blackwell GPUs connected through the GB200 NVL4 platform and NVIDIA Quantum‚ÄëX800 InfiniBand networking, strengthening Japan‚Äôs sovereign AI strategy and secure domestic infrastructure.

* AI for Science System: 1,600 Blackwell GPUs will power research in life sciences, materials science, climate and weather forecasting, manufacturing and laboratory automation.
* Quantum Computing System: 540 Blackwell GPUs will accelerate quantum algorithms, hybrid simulation and quantum‚Äëclassical methods.

The partnership builds on RIKEN‚Äôs collaboration with Fujitsu and NVIDIA to codesign FugakuNEXT, successor to the Fugaku supercomputer, expected to deliver 100x greater application performance and integrate production‚Äëlevel quantum computers by 2030.

The two new RIKEN systems are scheduled to be operational in spring 2026.

## **Arm Adopting NVIDIA NVLink Fusion [üîó](https://blogs.nvidia.com/blog/accelerated-computing-networking-supercomputing-ai/#arm)**

AI is reshaping data centers in a once-in-a-generation architectural shift, where efficiency per watt defines success. At the center is Arm Neoverse, deployed in over a billion cores and projected to reach 50% hyperscaler market share by 2025. Every major provider ‚Äî AWS, Google, Microsoft, Oracle and Meta ‚Äî is building on Neoverse, underscoring its role in powering AI at scale.

To meet surging demand, Arm is extending Neoverse with NVIDIA NVLink Fusion, the high-bandwidth, coherent interconnect first pioneered with Grace Blackwell. NVLink Fusion links CPUs, GPUs, and accelerators into one unified rack-scale architecture, removing memory and bandwidth bottlenecks that limit AI performance. Connected with Arm‚Äôs AMBA CHI C2C protocol, it ensures seamless data movement between Arm-based CPUs and partners‚Äô preferred accelerators.

Together, Arm and NVIDIA are setting a new standard for AI infrastructure, enabling ecosystem partners to build differentiated, energy-efficient systems that accelerate innovation across the AI era.

‚ÄúFolks building their own ARM CPU, or using an Arm IP can actually have access to NVLink Fusion, be able to connect that ARM CPU to an Nvidia GPU or to the rest of the NVLink ecosystem, and that‚Äôs happening at the racks and scale-up infrastructure,‚Äù said Buck.

## **Smarter Power for Accelerated Computing**

As AI factories scale, energy is becoming the new bottleneck. The
[NVIDIA Domain Power Service (DPS)](https://catalog.ngc.nvidia.com/orgs/nvidia/collections/dps)
flips that constraint into an opportunity ‚Äî turning power into a dynamic, orchestrated resource. Running as a Kubernetes service, DPS models and manages energy use across the data center, from rack to room to facility. It enables operators to extract more performance per megawatt by constraining power intelligently, improving throughput without expanding infrastructure.

DPS integrates tightly with the
[NVIDIA Omniverse DSX Blueprint](https://blogs.nvidia.com/blog/omniverse-dsx-blueprint/)
, a platform for designing and operating next-generation data centers. It works alongside technologies like Power Reservation Steering to balance workloads across the facility and the Workload Power Profile Solution to tune GPU power to the needs of specific jobs. Together, they form DSX Boost ‚Äî an energy-aware control layer that maximizes efficiency while meeting performance targets.

DPS also extends beyond the data center. With grid-facing APIs, it supports automated load shedding and demand response, helping utilities stabilize the grid during peak events. The result is a resilient, grid-interactive AI factory that turns every watt into measurable progress.