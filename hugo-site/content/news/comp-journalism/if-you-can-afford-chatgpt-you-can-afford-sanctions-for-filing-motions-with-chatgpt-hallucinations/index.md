---
ai_commentary: []
ai_commentary_meta:
  content_digest: ''
  generated_at: ''
  model: ''
  prompt_version: ''
  provider: ''
category: comp-journalism
date: '2025-12-06T12:03:31.472388+00:00'
exported_at: '2025-12-06T12:03:34.583672+00:00'
feed: https://reason.com/feed/
language: en
source_url: https://reason.com/volokh/2025/12/03/if-you-can-afford-chatgpt-you-can-afford-sanctions-for-filing-motions-with-chatgpt-hallucinations
structured_data:
  about: []
  author: ''
  description: 'From Jarrus v. Governor, decided yesterday by Judge F. Kay Behm (E.D.
    Mich.): The court is cognizant that imposing a...'
  headline: If You Can Afford ChatGPT, You Can Afford Sanctions for Filing Motions
    with ChatGPT Hallucinations
  inLanguage: en
  keywords: []
  main_image: ''
  original_source: https://reason.com/volokh/2025/12/03/if-you-can-afford-chatgpt-you-can-afford-sanctions-for-filing-motions-with-chatgpt-hallucinations
  publisher:
    logo: /favicon.ico
    name: gtcode.com
title: If You Can Afford ChatGPT, You Can Afford Sanctions for Filing Motions with
  ChatGPT Hallucinations
updated_at: '2025-12-06T12:03:31.472388+00:00'
url_hash: 5506d73fb81ac110291f0297d2e0c59bad029ae0
---

From
[*Jarrus v. Governor*](https://storage.courtlistener.com/recap/gov.uscourts.mied.384571/gov.uscourts.mied.384571.176.0.pdf)
, decided yesterday by Judge F. Kay Behm (E.D. Mich.):

> The court is cognizant that imposing a monetary sanction on plaintiffs who qualify for IFP status [based on inability to pay filing fees -EV] may be ineffective. However, the court also ordered Plaintiffs to explain how much, per month, they spend on "AI" subscriptions per month. Plaintiff Michael Jarrus explained that he pays approximately $20 per month in a subscription to "ChatGPT Plus." Over the course of 12 months, the evidence suggests that Plaintiff Michael Jarrus is at least able to afford a Chat GPT subscription of about $240. Absent proof that a monetary sanction will prove impossible to pay, the court will enter sanctions sufficient to deter similar conduct in the future….
>
> Consistent with Magistrate Judge Patti's warning that each AI citation might incur a cost of $200 per citation, the court adopts that amount and imposes a fine of $300 per Plaintiff (a total of $600) for three misrepresented, AI-generated citations.
> *Each Plaintiff [Michael Jarrus and his mother] shall, individually, be responsible for paying $300. These fines are due to the Clerk of Court and shall be paid in full by February 2, 2026. Failure to pay these amounts may result in dismissal of this action in its entirety or, if one Plaintiff pays their fine but not the other, of the nonpaying Plaintiff's claims for failure to comply….*
>
> If Plaintiffs file any future briefing in this case with even a single misrepresented, misquoted, or fictitious case that is caused by the use of generative "AI", this court will strongly consider any recommendation to dismiss this case for bad faith failure to comply with court orders, or revocation of Plaintiffs' IFP status, or in the alternative, it would not be clearly erroneous for the Magistrate Judge to strike or otherwise refuse to consider the merits of an entire briefing for the inclusion of a misrepresented, misquoted, or fictitious case.

More on the circumstances that led the court to be especially exercised here:

> Although Plaintiffs were warned by Magistrate Judge Patti of the dangers of the use of generative AI, Plaintiffs nonetheless appeared to make use of such a tool without checking the results.
> *See*
> ECF No. 170, PageID.3194 (including a clearly AI-generated phrase, "Here's the revised Paragraph 2, …"). Even on first review, their briefing appeared to be nothing more than a copy-paste from a chatbot-style generative AI tool. Ironically, Plaintiffs objected to the Magistrate Judge's warning about AI use in the same document that they relied on AI-generated text.
>
> Upon more careful review of the objections, the court identified a series of citations in Plaintiffs' brief that were false citations (with real cases, but with explanations that did not reflect the case cited). [Details omitted. -EV] …
>
> [T]he fact that Plaintiffs … did not "fabricate cases or cite nonexistent decisions" is of no help. When a case cite is "real," an attorney, or for that matter a judge, might see a case they recognize and assume the quote or holding has been accurately represented. That problem is illustrated here; although Chat GPT generated "holdings" that looked like they could plausibly have appeared in the cited cases, in fact it overstated their holdings to a significant degree. And while a litigant might get away with similar overstatements because they could, perhaps, reason their way to showing how a case's stated holding might extend to novel situations, an LLM
> *does not reason*
> in the way a litigant must.
>
> To put it in a slightly different way, LLMs do not perform the metacognitive processes that are necessary to comply with Rule 11. LLMs are tools that "emulate the communicative function of language, not the separate and distinct cognitive process of thinking and reasoning." When an LLM overstates a holding of a case, it is not because it made a mistake when logically working through how that case might represent a "nonfrivolous argument for extending, modifying, or reversing existing law or for establishing new law;" it is just piecing together a plausible-looking sentence—one whose content may or may not be true….